{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from fractions import Fraction\n",
    "from io import StringIO\n",
    "\n",
    "from sklearn import svm\n",
    "from scipy.sparse import csr_matrix, vstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestCorpusSet(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.negative = StringIO('I hated that so much')\n",
    "        self.negative_corpus = Corpus(self.negative, 'negative')\n",
    "        self.positive = StringIO('loved movie!! loved')\n",
    "        self.positive_corpus = Corpus(self.positive, 'positive')\n",
    "    \n",
    "    def test_trivial(self):\n",
    "        \"\"\"Consumes multiple files and turns it into sparse vector.\"\"\"\n",
    "        self.assertEqual('negative', self.negative_corpus.sentiment)\n",
    "        \n",
    "    def test_tokenize(self):\n",
    "        \"\"\"Downcases all the word tokens.\"\"\"\n",
    "        self.assertListEqual(['quick', 'brown', 'fox'],\n",
    "                             Corpus.tokenize('Quick Brown Fox'))\n",
    "    \n",
    "    def test_tokenize2(self):\n",
    "        \"\"\"Ignores all stop symbols.\"\"\"\n",
    "        self.assertListEqual(['hello'], \n",
    "                             Corpus.tokenize('\"\\'hello!?!?!.\\'\"   '))\n",
    "    \n",
    "    def test_tokenize3(self):\n",
    "        \"\"\"Ignores all unicode space.\"\"\"\n",
    "        self.assertListEqual(['hello', 'bob'], \n",
    "                             Corpus.tokenize(u'hello\\u00A0bob'))\n",
    "    \n",
    "    def test_positives(self):\n",
    "        \"\"\"Consumes a positive training set.\"\"\"\n",
    "        self.assertEqual('positive', \n",
    "                         self.positive_corpus.sentiment)\n",
    "    \n",
    "    def test_words(self):\n",
    "        \"\"\"Consumes a positive training set and unique set of words.\"\"\"\n",
    "        self.assertEqual({'loved', 'movie'}, \n",
    "                         self.positive_corpus.get_words())\n",
    "    \n",
    "    def test_sentiment_code_1(self):\n",
    "        \"\"\"Defines a sentiment_code of 1 for positives.\"\"\"\n",
    "        self.assertEqual(1, Corpus(StringIO(''), 'positive').sentiment_code)\n",
    "    \n",
    "    def test_sentiment_code_0(self):\n",
    "        \"\"\"Defines a sentiment code of -1 for negatives.\"\"\"\n",
    "        self.assertEqual(-1, Corpus(StringIO(''), 'negative').sentiment_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus:\n",
    "    skip_regex = re.compile(r'[\\'\"\\.\\?\\!]+')\n",
    "    space_regex = re.compile(r'\\s', re.UNICODE)\n",
    "    stop_words = []\n",
    "    stop_words = [x.strip() for x in open('./data/support_vector_machines/stopwords.txt').readlines()]\n",
    "    sentiment_to_number = {'positive': 1, 'negative': -1}\n",
    "    \n",
    "    @classmethod\n",
    "    def tokenize(cls, text):\n",
    "        cleared_text = cls.skip_regex.sub('', text)\n",
    "        parts = cls.space_regex.split(cleared_text)\n",
    "        parts = [part.lower() for part in parts]\n",
    "        return [p for p in parts if len(p) > 0 and p not in cls.stop_words]\n",
    "    \n",
    "    def __init__(self, io, sentiment):\n",
    "#         with open('./data/support_vector_machines/stopwords.txt') as f:\n",
    "#             self.stop_words = [x.strip() for x in f.readlines()]\n",
    "#         print(self.stop_words)\n",
    "        self._io = io\n",
    "        self._sentiment = sentiment\n",
    "        self._words = None\n",
    "    \n",
    "    @property\n",
    "    def sentiment(self):\n",
    "        return self._sentiment\n",
    "\n",
    "    @property\n",
    "    def sentiment_code(self):\n",
    "        return self.sentiment_to_number[self._sentiment]\n",
    "    \n",
    "    def get_words(self):\n",
    "        if self._words is None:\n",
    "            self._words = set()\n",
    "            for line in self._io:\n",
    "                for word in Corpus.tokenize(line):\n",
    "                    self._words.add(word)\n",
    "            self._io.seek(0)\n",
    "        return self._words\n",
    "    \n",
    "    def get_sentences(self):\n",
    "        for line in self._io:\n",
    "            yield line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestCorpusSet(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.positive = StringIO('I love this country')\n",
    "        self.negative = StringIO('I hate this man')\n",
    "        \n",
    "        self.positive_corp = Corpus(self.positive, 'positive')\n",
    "        self.negative_corp = Corpus(self.negative, 'negative')\n",
    "        \n",
    "        self.corpus_set = CorpusSet([self.positive_corp, self.negative_corp])\n",
    "        \n",
    "    def test_compose(self):\n",
    "        \"\"\"Composes two corpuses together.\"\"\"\n",
    "        self.assertEqual({'love', 'country', 'hate', 'man'},\n",
    "                         self.corpus_set.words)\n",
    "    \n",
    "    def test_sparse(self):\n",
    "        \"\"\"Returns a set of sparse vectors to train on.\"\"\"\n",
    "        expected_ys = [1, -1]\n",
    "        expected_xes = csr_matrix(np.array([\n",
    "            [1, 1, 0, 0],\n",
    "            [0, 0, 1, 1]\n",
    "        ]))\n",
    "        \n",
    "        self.corpus_set.calculate_sparse_vectors()\n",
    "        ys = self.corpus_set.yes\n",
    "        xes = self.corpus_set.xes\n",
    "        \n",
    "        self.assertListEqual(expected_ys, ys)\n",
    "        self.assertListEqual(list(expected_xes.data),\n",
    "                             list(xes.data))\n",
    "        self.assertListEqual(list(expected_xes.indices),\n",
    "                             list(xes.indices))\n",
    "        self.assertListEqual(list(expected_xes.indptr),\n",
    "                             list(xes.indptr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorpusSet:\n",
    "    def __init__(self, corpora):\n",
    "        self._yes = None\n",
    "        self._xes = None\n",
    "        self._corpora = corpora\n",
    "        self._words = set()\n",
    "        for corpus in self._corpora:\n",
    "            self._words.update(corpus.get_words())\n",
    "        \n",
    "    @property\n",
    "    def words(self):\n",
    "        return self._words\n",
    "    \n",
    "    @property\n",
    "    def xes(self):\n",
    "        return self._xes\n",
    "    \n",
    "    @property\n",
    "    def yes(self):\n",
    "        return self._yes\n",
    "    \n",
    "    def calculate_sparse_vectors(self):\n",
    "        self._yes = []\n",
    "        self._xes = None\n",
    "        for corpus in self._corpora:\n",
    "            vectors = self.feature_matrix(corpus)\n",
    "            if self._xes is None:\n",
    "                self._xes = vectors\n",
    "            else:\n",
    "                self._xes = vstack((self._xes, vectors))\n",
    "            self._yes.extend([corpus.sentiment_code] * vectors.shape[0])\n",
    "        \n",
    "    def feature_matrix(self, corpus):\n",
    "        data = []\n",
    "        indices = []\n",
    "        indptr = [0]\n",
    "        \n",
    "        for sentence in corpus.get_sentences():\n",
    "            sentence_indices = self._get_indices(sentence)\n",
    "            indices.extend(sentence_indices)\n",
    "            data.extend([1] * len(sentence_indices))\n",
    "            indptr.append(len(indices))\n",
    "            \n",
    "        feature_matrix = csr_matrix((data, indices, indptr), \n",
    "                                    shape=(len(indptr) - 1,\n",
    "                                           len(self._words)),\n",
    "                                    dtype=np.float64)\n",
    "        feature_matrix.sort_indices()\n",
    "        return feature_matrix\n",
    "    \n",
    "    def feature_vector(self, sentence):\n",
    "        indices = self._get_indices(sentence)\n",
    "        data = [1] * len(indices)\n",
    "        indptr = [0, len(indices)]\n",
    "        vector = csr_matrix((data, indices, indptr),\n",
    "                            shape=(1, len(self._words)),\n",
    "                            dtype=np.float64)\n",
    "        return vector\n",
    "    \n",
    "    def _get_indices(self, sentence):\n",
    "        word_list = list(self._words)\n",
    "        indices = []\n",
    "        for token in Corpus.tokenize(sentence):\n",
    "            if token in self._words:\n",
    "                index = word_list.index(token)\n",
    "                indices.append(index)\n",
    "        return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_DATA = './data/support_vector_machines/rt-polaritydata/rt-polarity.pos'\n",
    "NEG_DATA = './data/support_vector_machines/rt-polaritydata/rt-polarity.neg'\n",
    "\n",
    "class TestSentimentClassifier(unittest.TestCase):\n",
    "    def setUp(self): \n",
    "        pass\n",
    "    \n",
    "    def test_validate(self):\n",
    "        \"\"\"Cross validate with an error of 35% or less.\"\"\"\n",
    "        neg = self.split_file(POS_DATA)\n",
    "        pos = self.split_file(NET_DATA)\n",
    "        \n",
    "        classifier = SentimentClassifier.build([\n",
    "            neg['training'],\n",
    "            pos['training']\n",
    "        ])\n",
    "        c = 2 ** 7\n",
    "        classifier.c = c\n",
    "        classifier.reset_model()\n",
    "        \n",
    "        n_er = self.validate(classifier, neg['validation'], 'negative')\n",
    "        p_er = self.validate(classifier, pos['validation'], 'positive')\n",
    "        total = Fraction(n_er.numerator + p_er.numerator,\n",
    "                         n_er.denominator + p_er.denominator)\n",
    "        print(total)\n",
    "        self.assertLess(total, 0.35)\n",
    "    \n",
    "    def test_validate_itself(self):\n",
    "        \"\"\"Yields a zero error when it uses itself.\"\"\"\n",
    "        classifier = SentimentClassifier.build([\n",
    "            POS_DATA, NEG_DATA\n",
    "        ])\n",
    "        \n",
    "        c = 2 ** 7\n",
    "        classifier.c = c\n",
    "        classifier.reset_model()\n",
    "        \n",
    "        n_er = self.validate(classifier, NEG_DATA, 'negative')\n",
    "        p_er = self.validate(classifier, POS_DATA, 'positive')\n",
    "        total = Fraction(n_er.numerator + p_er.numerator,\n",
    "                         n_er.denominator + p_er.denominator)\n",
    "        \n",
    "        print(total)\n",
    "        self.assertEqual(total, 0)\n",
    "        \n",
    "    def validate(self, classifier, file, sentiment):\n",
    "        total = 0\n",
    "        misses = 0\n",
    "        \n",
    "        with open(file, 'r', encoding='latin-1') as f:\n",
    "            for line in f:\n",
    "                if classifier.classify(line) != sentiment:\n",
    "                    misses += 1\n",
    "                total += 1\n",
    "        return Fraction(misses, total)\n",
    "\n",
    "    def split_file(self, filepath):\n",
    "        ext = os.path.splitext(filepath)[1]\n",
    "        \n",
    "        counter = 0\n",
    "        training_filename = './data/support_vector_machines/training%s' % ext\n",
    "        validation_filename = './data/support_vector_machines/validation%s' % ext\n",
    "        \n",
    "        with open(filepath, 'r', encoding='latin-1') as input_file:\n",
    "            with open(validation_filename, 'w') as val_file:\n",
    "                with open(training_filename, 'w') as train_file:\n",
    "                    for line in input_file:\n",
    "                        if counter % 2 == 0:\n",
    "                            val_file.write(line)\n",
    "                        else:\n",
    "                            train_file.write(line)\n",
    "                        counter += 1\n",
    "        return {'training': training_filename,\n",
    "                'validation': validation_filename}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentClassifier(object):\n",
    "    ext_to_sentiment = {'.pos': 'positive',\n",
    "                        '.neg': 'negative'}\n",
    "    number_to_sentiment = {-1: 'negative',\n",
    "                           1: 'positive'}\n",
    "    \n",
    "    @classmethod\n",
    "    def present_answer(cls, answer):\n",
    "        if isinstance(answer, ndarray):\n",
    "            answer = answer[0]\n",
    "        return cls.number_to_sentiment[answer]\n",
    "    \n",
    "    @classmethod\n",
    "    def build(cls, files):\n",
    "        corpora = []\n",
    "        for file in files:\n",
    "            ext = os.path.splitext(file)[1]\n",
    "            corpus = Corpus(open(file, 'r', encoding='latin-1'), cls.ext_to_sentiment[ext])\n",
    "            corpora.append(corpus)\n",
    "        corpus_set = CorpusSet(corpora)\n",
    "        return SentimentClassifier(corpus_set)\n",
    "\n",
    "    def __init__(self, corpus_set):\n",
    "        self._trained = False\n",
    "        self._corpus_set = corpus_set\n",
    "        self._c = 2 ** 7\n",
    "        self._model = None\n",
    "    \n",
    "    @property\n",
    "    def c(self):\n",
    "        return self._c\n",
    "    \n",
    "    @c.setter\n",
    "    def c(self, cc):\n",
    "        self._c = cc\n",
    "        \n",
    "    def reset_model(self):\n",
    "        self._model = None\n",
    "        \n",
    "    def words(self):\n",
    "        return self._corpus_set.words\n",
    "    \n",
    "    def classify(self, string):\n",
    "        if self._model is None:\n",
    "            self._model = self.fit_model()\n",
    "        prediction = self._model.predict(self._corpus_set.feature_vector(string))\n",
    "        return self.present_answer(prediction)\n",
    "\n",
    "    def fit_model(self):\n",
    "        self._corpus_set.calculate_sparse_vectors()\n",
    "        y_vec = self._corpus_set.yes\n",
    "        x_mat = self._corpus_set.xes\n",
    "        clf = svm.SVC(C=self.c,\n",
    "                      cache_size=1000,\n",
    "                      gamma=1.0/len(y_vec),\n",
    "                      kernel='linear',\n",
    "                      total=0.001)\n",
    "        clf.fit(x_mat, y_vec)\n",
    "        return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".FE"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=['excluded'], exit=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
